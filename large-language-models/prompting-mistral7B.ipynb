{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6602da5-3c8d-40dd-9211-a0009357d69a",
   "metadata": {},
   "source": [
    "# Experimenting Prompt Engineering - Chatbot\n",
    "\n",
    "In this notebook we will look at a few prompt engineering techniques. We will experiment by loading a 7 billion parameter Large Language Model (LLM) within the notebook environment itself and throwing some prompts its way to see what we can make it do.\n",
    "\n",
    "After trying a few different prompts, we will run a simple chatbot using the prompt engineering techniques we explored. \n",
    "\n",
    "At the end, I will list some pointers in case you would like to build on this code, by dropping in other LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fdae7-bb30-4e60-a2ec-01eaf20657e2",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/blob/main/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run for free on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/) with CPU.  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]\n",
    "\n",
    "> Whatever environment you end up using, make sure you have at least 12 GB of disk space available to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8a19a-8522-4bcf-b2a5-54cfcad0aead",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First, if needed, install `ctransformers` - a library based on `transformers` from [Hugging Face](https://huggingface.co/), a great open source set of libraries for working and experimenting with the underlying technology of generative AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa1ac45-da8a-441e-83d6-65e8132bba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ctransformers>=0.2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bacffd8-a503-4a92-b344-fc8139495ee2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.331-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: anyio<4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (2.31.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 39.5 MB/s eta 0:00:01��██████████████        | 798 kB 39.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2,>=1\n",
      "  Downloading numpy-1.26.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.2 MB 78.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.52\n",
      "  Downloading langsmith-0.0.60-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[K     |████████████████████████████████| 395 kB 77.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 100.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 84.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\n",
      "\u001b[K     |████████████████████████████████| 228 kB 96.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.1.2)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Collecting pydantic-core==2.10.1\n",
      "  Downloading pydantic_core-2.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (610 kB)\n",
      "\u001b[K     |████████████████████████████████| 610 kB 79.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pydantic-core, mypy-extensions, multidict, frozenlist, annotated-types, yarl, typing-inspect, pydantic, marshmallow, greenlet, async-timeout, aiosignal, tenacity, SQLAlchemy, numpy, langsmith, jsonpatch, dataclasses-json, aiohttp, langchain\n",
      "Successfully installed SQLAlchemy-2.0.23 aiohttp-3.8.6 aiosignal-1.3.1 annotated-types-0.6.0 async-timeout-4.0.3 dataclasses-json-0.6.1 frozenlist-1.4.0 greenlet-3.0.1 jsonpatch-1.33 langchain-0.0.331 langsmith-0.0.60 marshmallow-3.20.1 multidict-6.0.4 mypy-extensions-1.0.0 numpy-1.26.1 pydantic-2.4.2 pydantic-core-2.10.1 tenacity-8.2.3 typing-inspect-0.9.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dc104e-d8e9-489b-ab21-dbcecbc3ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04e3ec-9978-45f5-9a00-e41ba50804f4",
   "metadata": {},
   "source": [
    "## Loading Mistral-7B-OpenOrca Model\n",
    "\n",
    "The [Mistral-7B-OpenOrca](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF) model was fine-tuned on top of Mistral 7B using OpenOrca dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be93f04-fcca-4588-ac3d-ec9202e3b214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee09d601fab45f0b0379aaeb534c762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb727f9f3d64715a1d56de9e52dda65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)a0bba72a0abe8aea7e127d7994cb/config.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c5cff567bb42f9811d6b5556505a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09fe0c2ad604aa1802a5a96a00fb430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-7b-openorca.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-OpenOrca-GGUF\", model_file=\"mistral-7b-openorca.Q4_K_M.gguf\", model_type=\"mistral\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31903-630d-4b12-bea0-a77a872b61ac",
   "metadata": {},
   "source": [
    "# How do large language models work?\n",
    "\n",
    "## Prompt engineering\n",
    "\n",
    "LLMs are trained to predict next word, given a sequence of words. See [2nd diagram](https://huggingface.co/docs/transformers/main/llm_tutorial) on HF tutorial which demonstrates the concept. Let's try calling Mistral model and ask it to describe a concept:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e42ffa-f7be-46c3-bd07-117ecf5dabf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Photosynthesis is a process by which green plants and some other organisms use sunlight to synthesize or make their food (glucose) from carbon dioxide and water. This process occurs in the chloroplasts of plant cells, where light energy is converted into chemical energy.\n",
      "\n",
      "The photosynthesis process can be divided into two main stages:\n",
      "\n",
      "1. The Light-dependent reactions (Light phase): In this stage, the absorbed light energy is used to produce ATP (Adenosine Triphosphate) and NADPH (Nicotinamide adenine dinucleotide phosphate), which are high-energy molecules that can be used later in the Calvin cycle.\n",
      "\n",
      "2. The Light-independent reactions (Calvin Cycle): In this stage, ATP and NADPH produced during the light-dependent reactions are used to convert carbon dioxide into glucose through a series of chemical reactions. This process does not require light energy but depends on the products generated in the previous stage.\n",
      "\n",
      "Photosynthesis is essential for life on Earth because it provides oxygen, which is necessary\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Explain photosythesis\",max_new_tokens=250, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335c8f-ede7-4a65-813e-ec225822d5db",
   "metadata": {},
   "source": [
    "---\n",
    "The model did pretty well, let's see if we can make the answer more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fe60fe-bc65-4855-819e-a7ecb3071e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Photosynthesis is the process by which green plants, algae, and some bacteria convert sunlight into chemical energy in the form of glucose or other sugars. This process involves two main stages: the light-dependent reactions and the light-independent reactions (also known as the Calvin cycle).\n",
      "\n",
      "In the first stage, light-dependent reactions, chlorophyll (a green pigment found in plant cells) absorbs sunlight and uses it to generate energy. This energy is used to produce ATP (adenosine triphosphate), which is a source of chemical energy for the cell, and NADPH (nicotinamide adenine dinucleotide ph\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(llm(\"Explain photosythesis in three sentences.\",max_new_tokens=150, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7071987-b179-4cd2-900c-28e02894c7d3",
   "metadata": {},
   "source": [
    "---\n",
    "Better; however, the model didn't answer in three sentences. Let's try a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d3bbb-39f1-4089-bfb0-6d07931a8c7d",
   "metadata": {},
   "source": [
    "## Few-shot prompts\n",
    "We can influence model's output style by using what's called few-shot prompting - a technique which shows the model the exact behavior we expect on few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e991dc-6f28-480d-837c-267b1e238015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Photosynthesis is a process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll pigments. During this process, light energy is converted into chemical energy that can later be released to fuel the organisms' activities. In photosynthesis, carbon dioxide and water are absorbed from the atmosphere and a carbohydrate and oxygen are produced.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"Explain precipitation in two sentences.\n",
    "In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail. \n",
    "--\n",
    "Explain condensation in one sentence.\n",
    "Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.\n",
    "--\n",
    "Explain photosynthesis in three sentences.\"\"\", max_new_tokens=150, temperature=0.1, stop=\"--\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc398e-65a6-4ed6-99bc-19a3f2fa0498",
   "metadata": {},
   "source": [
    "---\n",
    "Great, it worked! You will notice we separated each example with double-dash sign and also signaled to LLM that it's a stop sequence, meaning, if it gets to the point where it wants to generate dash dash (because it's a pattern we showed it), it must stop.\n",
    "# Instruction tuning\n",
    "\n",
    "Building complex prompts to achieve simple objectives like this one can get complicated very fast, especially once we attempt more sophisticated tasks. Luckily, LLMs can be instruction tuned. This is done on special [datasets](https://huggingface.co/datasets/Open-Orca/OpenOrca) which fine-tune the model to follow directions as closely as possible. In other words, instead of providing examples in each prompt, this is done by changing the model weights as part of training processs. This Mistral model was actually instruction tuned and we just need to follow the format which was used during training:\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433268b4-a875-446a-bf1a-fc4237d59bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Photosynthesis is a process in which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (food) and oxygen. This process occurs in the chloroplasts of green plants and involves two stages: the light-dependent reactions and the light-independent reactions (Calvin cycle). Photosynthesis is essential for life on Earth as it provides energy, food, and oxygen to living organisms.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
    "<|im_start|>user\n",
    "Explain photosynthesis in just three sentences<|im_end|>\n",
    "<|im_start|>assistant\"\"\",max_new_tokens=150, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046f28b-a89f-41f5-9d54-89263a396039",
   "metadata": {},
   "source": [
    "# Dyanamic prompting\n",
    "Now that we know how to submit instructions to Mistral LLM, let's try assembling prompts dynamically - part of the prompt can be fixed, and part can be provided on the fly. We achieve this by creating prompt template with variables. The value for each variable is supplied in a separate statement and that statement can be executed further down in the code. Basically, this is a way to de-couple static part of the prompt from variable one, making the entire prompt dynamic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da5c5b3-3d24-496a-88cb-27a20b468275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the content below, answer the question that follows. \n",
      "Content: ABC stands for Absolute Best Computer\n",
      "Question: What does ABC stand for?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"<|im_start|>system\n",
    "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
    "<|im_start|>user\n",
    "Given the content below, answer the question that follows. \n",
    "Content: {content}\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    ")\n",
    "prompt=prompt_template.format(content=\"ABC stands for Absolute Best Computer\", question=\"What does ABC stand for?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ae3a2e-ce93-4240-bd85-8fab25c7eb55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The acronym \"ABC\" in the given content stands for \"Absolute Best Computer\".'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt,max_new_tokens=150, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7dba57c-19dc-487f-8e4f-6470f1a8bcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe term \"ABC\" is an acronym that stands for \"Alcohol, Beverages, and Cigarettes.\" It refers to the three primary categories of products that are commonly sold in convenience stores, supermarkets, and other retail establishments. These items are often displayed prominently at the front of the store or in a separate section, as they are high-margin products that generate significant revenue for the retailer.\\n\\nAlcohol: This category includes beer, wine, and spirits such as liquor and distilled beverages. Alcoholic beverages can be further divided into various types, including domestic and imported brands, as well as different styles of drinks like lagers, ales,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And what if we run it without any context:\n",
    "llm(\"What does ABC stand for?\",max_new_tokens=150, temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37d540-d260-4ccb-aa0a-00ee6f6f71e2",
   "metadata": {},
   "source": [
    "---\n",
    "In the example above you can see how without provided context, LLM uses its own \"knowledge\" to answer questions. The approach of providing context is used heavily in RAG (Retrieval Augmented Generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd190db-6d79-4a00-9d67-b0bfb4019a3f",
   "metadata": {},
   "source": [
    "## More examples of LLM use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7671b-87da-453b-a400-6f125d644b38",
   "metadata": {},
   "source": [
    "For simpler questions/prompts we can often get away witout strict instruction formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c507742b-dab9-4b97-b2c2-13f7cbfc1688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe students should use their notebooks to practice installing required libraries and validating the code runs without any errors.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"\"\"The following text came from OCR, correct obvious mistakes:\n",
    "The students should use their notebots to practice installing required libraries and validating the code runs without any errands.\n",
    "\"\"\",max_new_tokens=150, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77da66fb-3eb8-4fe6-8744-9c90957f4088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe correct version of the sentence is:\\n\"I believe I would like to apply for this position, but I am unsure about the process; could you offer some guidance?\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"\"\"Rewrite the following sentence in better English:\n",
    "I think I want to apply for this position but don't know how, can you help?\n",
    "\"\"\",max_new_tokens=150, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71e5cb14-1bbe-4ff3-a570-f2cb25742d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI think the error is because you are trying to import a module called \"zip\" which does not exist in Python. The \"zip\" function is a built-in function in Python and does not need to be imported as a module. If you want to use the zip function, you can simply call it directly without importing any module.\\n\\nFor example:\\n```python\\na = [1, 2, 3]\\nb = [4, 5, 6]\\nc = list(zip(a, b))\\nprint(c)\\n```\\nThis code will output `[(1, 4), (2, 5), (3, 6)]`.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"\"\"I am getting the following error when attempting to run this pythong code, can you explain why?\n",
    "  File \"/home/studio-lab-user/sagemaker-studiolab-notebooks/ha.py\", line 2, in <module>\n",
    "    import zip\n",
    "ModuleNotFoundError: No module named 'zip'\n",
    "\"\"\",max_new_tokens=150, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f84be33-49d8-463a-99ae-44b099b28da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReducted Paragraph:\\nJeff Bezos lives at 1 Main St. Miami, FL ###. His phone number is 111-123-4567.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"\"\"Reduct PII from the following paragraph. Replace any PII with ###.\n",
    "Paragraph:\n",
    "Jeff Bezos lives at 1 Main St. Miami, FL 39812. His phone number is 111-123-4567.\n",
    "\"\"\",max_new_tokens=150, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a806d-7351-408c-b141-fee35b1853b7",
   "metadata": {},
   "source": [
    "---\n",
    "The last one or two examples didn't quite work. Perhaps the model is not strong enough for this type of task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b86c6b2f-da12-4fef-b1e7-3597610d4e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"\"\"Create multiple choice question to test student's understanding of photosynthesis. The question must have at least three distractors. Indicate the correct answer.\n",
    "\"\"\",max_new_tokens=250, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02269b-5c43-4e13-8722-c5284a9ab978",
   "metadata": {},
   "source": [
    "---\n",
    "The above didn't work at all. Let's try with better prompt formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31117af-933d-4152-81a1-93572f209a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question: In the process of photosynthesis, which molecule is converted into glucose?\n",
      "\n",
      "A) Carbon dioxide (CO2)\n",
      "B) Oxygen (O2)\n",
      "C) Water (H2O)\n",
      "D) Glucose (C6H12O6)\n",
      "\n",
      "Correct Answer: D) Glucose (C6H12O6)\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant which creates assessments to help educators evaluate students knowledge<|im_end|>\n",
    "<|im_start|>user\n",
    "Create multiple choice question to test student's understanding of photosynthesis. The question must have at least three distractors. Indicate the correct answer.<|im_end|>\n",
    "<|im_start|>assistant\"\"\",max_new_tokens=250, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd469ce-4e37-4cec-8168-cadd5d072237",
   "metadata": {},
   "source": [
    "# Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c3685e-03e7-4072-bc8c-8a67b63aaa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      " Amazon Web Services (AWS) has not specifically announced a new chemical solution at their re:Invent event. However, they have introduced various new services and features in different domains such as compute, storage, networking, databases, security, and more. Some of the notable announcements from the 2021 re:Invent event include:\n",
      "\n",
      "1. Amazon SageMaker Studio Notebooks: A fully managed development environment for data scientists to build, train, and deploy machine learning models.\n",
      "\n",
      "2. AWS Outposts: An on-premises hardware solution that brings native AWS services, infrastructure, and operating models to customer datacenters or edge locations.\n",
      "\n",
      "3. Amazon SageMaker Canary: A feature that allows data scientists to gradually roll out their machine learning models in production while monitoring the performance and impact on end-users.\n",
      "\n",
      "4. AWS Security Hub: A centralized platform for managing and organizing security alerts from multiple AWS services, third-party security tools, and automating security best practices.\n",
      "\n",
      "5. Amazon EMR on Amazon Elastic Kubernetes Service (EKS): Enables customers to run big data applications on Kubernetes clusters\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant which answers user questions politely and in great details<|im_end|>\n",
    "<|im_start|>user\n",
    "What type of new chemical solution AWS announced at reInvent this year?<|im_end|>\n",
    "<|im_start|>assistant\"\"\",max_new_tokens=250, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666f316-d20c-4788-9c44-31b3035bd5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant which answers user questions politely and in great details. Do not make up facts. Say I don't know if you have no information about something.<|im_end|>\n",
    "<|im_start|>user\n",
    "What type of new chemical solution AWS announced at reInvent this year?<|im_end|>\n",
    "<|im_start|>assistant\"\"\",max_new_tokens=250, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb206-6661-4ba6-95c5-6369176ff869",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "Let's make a simple chatbot.  There is no special library to include and no setting to apply to the LLM, all we need is prompt engineering!\n",
    "\n",
    "Let's use what we know of prompts with in context learning, to create a simple chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "898d619d-6c08-46d4-a0ba-cb1a9e23a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Jeff Bezos is the founder, chairman, and former CEO of Amazon.com, an e-commerce giant and technology company. He is also the owner of The Washington Post and has invested in various other businesses and ventures. Born on January 12, 1964, he is one of the richest people in the world with a significant influence on global business and technology.\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant chat bot which answers user questions concisely and stops<|im_end|>\n",
    "<|im_start|>user\n",
    "Who is Jeff Bezos?<|im_end|>\n",
    "<|im_start|>assistant\"\"\",max_new_tokens=150, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae5b02ab-f7d3-4022-b26d-85284e8fe9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": I cannot determine someone's age without knowing their birthdate. Please provide the person's birthdate, or let me know if you would like to ask about a different topic.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant chat bot which answers user questions concisely and stops<|im_end|>\n",
    "<|im_start|>user\n",
    "How old is he?<|im_end|>\n",
    "<|im_start|>assistant\"\"\",max_new_tokens=150, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4500db7-a2da-40ce-b525-a33eacb46c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jeff Bezos was born on January 12, 1964. As of now, his age can be calculated by subtracting his birth year from the current year. Please note that this information may change as he ages.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"\"\"<|im_start|>system\n",
    "You are an AI assistant chat bot which answers user questions concisely and stops<|im_end|>\n",
    "<|im_start|>user\n",
    "Given the following context, answer the question below.\n",
    "Context: Who is Jeff Bezos? Jeff Bezos is the founder, chairman, and former CEO of Amazon.com, an e-commerce giant and technology company. He is also the owner of The Washington Post and has invested in various other businesses and ventures. Born on January 12, 1964, he is one of the richest people in the world with a significant influence on global business and technology.\n",
    "Question: How old is he?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",max_new_tokens=150, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac46a54-7a46-4e0d-b089-553106c1b25f",
   "metadata": {},
   "source": [
    "## Want to use a different model with this notebook?\n",
    "\n",
    "Hugging Face have many models that you can use and drop in to code like this. But you may need to make  modifications depending on the model you choose. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc48471-8e5e-4646-95c9-650771a050c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571ecac-f572-49a2-921c-61357af52302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
